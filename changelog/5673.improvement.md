Return diagnostic data for action and NLU predictions.

DIET and TED now both expose the attention weights of their transformer layers.
This can be used for debugging and fine-tuning, e.g. with RasaLit.
