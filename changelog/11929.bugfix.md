Exit training when transformer_size is not divisible by the number_of_attention_heads parameter and update the transformer documentations.