---
id: getting-started-with-analytics
sidebar_label: Getting started
title: Getting started with Analytics
description:
abstract: Rasa Pro Analytics is a data pipeline that extracts model, training,
  and conversation data from the Rasa platform and streams the data to
  your data warehouse for analysis.
---

import useBaseUrl from "@docusaurus/useBaseUrl";
import RasaProLabel from "@theme/RasaProLabel";
import RasaProBanner from "@theme/RasaProBanner";
import {Grid} from "@theme/Grid"

<RasaProLabel />

<RasaProBanner />

Analytics helps visualize and process Rasa assistant metrics in the
tooling (BI tools, data warehouses) of your choice. Visualizations
and analysis of the production assistant and its conversations allow
you to assess ROI and improve the performance of the assistant over time.

<Grid  columns="1fr 1fr" noMargin noGap verticalAlign="middle">
  <img
    alt="Rasa Pro Analytics - Sessions per channel"
    src={useBaseUrl("/img/analytics/graph-number-sessions-channel.png")}
    width="100%"
  />
  <img
    alt="Rasa Pro Analytics - Escalation rate"
    src={useBaseUrl("/img/analytics/graph-escalation-rate.png")}
    width="100%"
  />
  <img
    alt="Rasa Pro Analytics - Intents distribution over time"
    src={useBaseUrl("/img/analytics/graph-intent-distribution.png")}
    width="100%"
  />
  <img
    alt="Rasa Pro Analytics - Top intents"
    src={useBaseUrl("/img/analytics/graph-top-5-intents.png")}
    width="100%"
  />
</Grid>

Measuring a Rasa assistant's success and making strategic investments
will lead to better business outcomes. It will also help you understand
the needs of your end users and better serve them.

<div align="center">
  <img
    alt="An overview of the components of Rasa Pro."
    src={useBaseUrl("/img/rasa_pro_overview.png")}
    width="100%"
  />
</div>

Rasa Pro will connect to your production assistant using Kafka.
The pipeline will compute analytics as soon as the docker container is successfully
deployed and connected to your data warehouse and Kafka instances.


## Prerequisites

- A production deployment of Kafka is required to set up Rasa Pro.
  We recommend using [Amazon Managed Streaming for
  Apache Kafka](https://aws.amazon.com/msk/).
- A production deployment of a data lake needs to be connected to
  the data pipeline. Rasa Pro supports the following data lakes:

  - [PostgreSQL](https://aws.amazon.com/rds/postgresql/) (**recommended**. All PostgreSQL >= 11.0 are supported)
  - [Amazon Redshift](https://aws.amazon.com/redshift/)

  We recommend managed deployments of your data lake to minimize maintenance
  efforts.

## Connect an assistant

The `<DATA_LAKE_URL>` should have a DBAPI format, e.g.
`postgresql+psycopg2://user:password@postgres.host:5432/rasa`.
If no data lake URL is configured, Analytics will be disabled.

`RASA_ANALYTICS_DB_URL=<DATA_LAKE_URL>`

## Connect a data warehouse

You can choose to configure the analytics pipeline to stream the transformed conversational data to two different data warehouse types in AWS:

- [PostgreSQL](#postgresql)
- [Redshift](#redshift)


### PostgreSQL

You can use Amazon Relational Database Service (RDS) to create a PostgreSQL DB instance which is the environment that will run your PostgreSQL database.

First you must set up Amazon RDS by completing the instructions listed [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SettingUp.html).
Next, create the PostgreSQL DB instance. You can follow one of the following instruction sets:

- the AWS *Easy create* instructions listed in the [**Creating a PostgreSQL DB instance** section](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html#CHAP_GettingStarted.Creating.PostgreSQL)
- the AWS [*Standard create* instructions](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateDBInstance.html)

To build the DB URL in the DBAPI format specified in the [**Connect an assistance** section](#connect-an-assistant) you must enter the database credentials.
You must obtain the database username and password after you select a database authentication option during the process of the PostgreSQL DB instance creation:

- **Password authentication** to use database credentials only, in which case you must enter a username for the master username, as well as generate the master password.
- [**Password and IAM DB authentication**](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html) to use IAM users and roles for authentication of database users.
- [**Password and Kerberos authentication**](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/postgresql-kerberos.html)

Finally, when running the Analytics pipeline Docker container, set the [environment variable `RASA_ANALYTICS_DB_URL`](../../deploy/deploy-rasa-pro-services.mdx#docker-container-configuration-reference) to the PostgreSQL Amazon RDS DB instance URL.

### Redshift

If you prefer instead to set up Amazon Redshift as your choice of data lake, you can choose to either stream the data from the
PostgreSQL source database within the Amazon RDS DB instance created at the [**PostgreSQL step**](#postgresql) to the Redshift target or [connect directly](#direct-connection) to Amazon Redshift.

#### Streaming from PostgreSQL to Redshift

If you meet the [prerequisites](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites) for using an Amazon Redshift database as a target, you will need to implement two steps:

1. configure the PostgreSQL source for AWS Database Migration Service (DMS) by following these [instructions](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html)
2. configure the Redshift target for AWS DMS following the instructions [here](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html).

#### Direct connection

You can get started on enabling the direct connection with Redshift by following these resources on creating an [Amazon Redshift cluster](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html).
Before you do so, you should take into account that streaming historical data directly to Redshift could take much longer than streaming directly to or via the PostgreSQL RDS instance.

You must update the `RASA_ANALYTICS_DB_URL` to the Redshift cluster DB URL which must follow the format `redshift://<USER>:<PASSWORD>@<AWS URL>:5439/<DB NAME>`,  e.g. `redshift://awsuser:4324312adfaGQ@analytics.cp1yucixmagz.us-east-1.redshift.amazonaws.com:5439/analytics`.

:::caution Redshift write performance

As a result of performance considerations, we strongly advise against choosing a direct connection to Redshift.
Redshift is a great data lake for analytics but lacks the necessary write performance to directly stream data to it.

:::

## Ingest past conversations

When Analytics is connected to your Kafka instance, it will consume
all prior events on the Kafka topic and ingest them into the database.
Kafka has a retention policy for events on a [topic which defaults to
7 days](https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html#topicconfigs_retention.ms).

If you want to process events from conversations that are older than the
retention policy configured for the Rasa topic, you can manually
ingest events from past conversations.

Manually ingesting data from past conversations requires a connection to the
tracker store. The tracker store contains past conversations and a
connection to the Kafka cluster. Use the `rasa export` command to export
the events stored in the tracker store to Kafka:

```shell
rasa export --endpoints endpoints.yml
```

Configure the export to read from your production tracker store
and write to Kafka as an event broker, e.g.

```yaml-rasa title="endpoints.yml"
  tracker_store:
      type: SQL
      dialect: "postgresql"
      url: "localhost"
      db: "tracker"
      username: postgres
      password: password

  event_broker:
      type: kafka
      topic: rasa-events
      url: localhost:29092
      partition_by_sender: true
```

:::note

Running manual ingestion of past events multiple times will result in
duplicated events. There is currently no deduplication implemented in
Analytics. Every ingested event will be stored in the database,
even if it was processed previously.
:::

## Connect a BI Solution

Connecting a business intelligence platform to the data warehouse varies for
each platform. We provide example instructions for Metabase and Tableau but
you can use any BI platform which supports AWS Redshift or PostgreSQL.

### Example: Connecting Metabase

Metabase is a free and open-source business intelligence platform. It
provides a simple interface to query and visualize data. Metabase can
be connected to PostgreSQL or Redshift databases.

- [Connecting Metabase to PostgreSQL](https://www.metabase.com/data_sources/postgresql)
- [Connecting Metabase to Redshift](https://www.metabase.com/data_sources/amazon-redshift)

### Example: Connecting Tableau

Tableau is a business intelligence platform. It provides a flexible interface
to build business intelligence dashboards. Tableau can be connected to
PostgreSQL or Redshift databases.

- [Connecting Tableau to PostgreSQL](https://help.tableau.com/current/pro/desktop/en-us/examples_postgresql.htm)
- [Connecting Tableau to Redshift](https://help.tableau.com/current/pro/desktop/en-us/examples_amazonredshift.htm)
