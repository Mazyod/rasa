---
id: language-models
sidebar_label: Language Models
title: Language Models
---

The following components load pre-trained models that are needed if you want to use pre-trained
word vectors in your pipeline.

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="mitienlp"></a>

## MitieNLP


* **Short**

  MITIE initializer



* **Outputs**

  Nothing



* **Requires**

  Nothing



* **Description**

  Initializes MITIE structures. Every MITIE component relies on this,
  hence this should be put at the beginning
  of every pipeline that uses any MITIE components.



* **Configuration**

  The MITIE library needs a language model file, that **must** be specified in
  the configuration:

  ```yaml
  pipeline:
  - name: "MitieNLP"
    # language model to load
    model: "data/total_word_feature_extractor.dat"
  ```

  For more information where to get that file from, head over to
  [installing MITIE](../user-guide/installation#install-mitie).


<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="spacynlp"></a>

## SpacyNLP


* **Short**

  spaCy language initializer



* **Outputs**

  Nothing



* **Requires**

  Nothing



* **Description**

  Initializes spaCy structures. Every spaCy component relies on this, hence this should be put at the beginning
  of every pipeline that uses any spaCy components.



* **Configuration**

  You need to specify the language model to use.
  By default the language configured in the pipeline will be used as the language model name.
  If the spaCy model to be used has a name that is different from the language tag (`"en"`, `"de"`, etc.),
  the model name can be specified using the configuration variable `model`.
  The name will be passed to `spacy.load(name)`.

  ```yaml
  pipeline:
  - name: "SpacyNLP"
    # language model to load
    model: "en_core_web_md"

    # when retrieving word vectors, this will decide if the casing
    # of the word is relevant. E.g. `hello` and `Hello` will
    # retrieve the same vector, if set to `False`. For some
    # applications and models it makes sense to differentiate
    # between these two words, therefore setting this to `True`.
    case_sensitive: False
  ```

  For more information on how to download the spaCy models, head over to
  [installing SpaCy](../user-guide/installation#install-spacy).


<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="hftransformersnlp"></a>

## HFTransformersNLP


* **Short**

  HuggingFace’s Transformers based pre-trained language model initializer



* **Outputs**

  Nothing



* **Requires**

  Nothing



* **Description**

  Initializes specified pre-trained language model from HuggingFace’s [Transformers library](https://huggingface.co/transformers/).  The component applies language model specific tokenization and
  featurization to compute sequence and sentence level representations for each example in the training data.
  Include [LanguageModelTokenizer](./components#languagemodeltokenizer) and [LanguageModelFeaturizer](./components#languagemodelfeaturizer) to utilize the output of this
  component for downstream NLU models.

  :::note
  To use `HFTransformersNLP` component, install Rasa Open Source with `pip install rasa[transformers]`.

  :::



* **Configuration**

  You should specify what language model to load via the parameter `model_name`. See the below table for the
  available language models.
  Additionally, you can also specify the architecture variation of the chosen language model by specifying the
  parameter `model_weights`.
  The full list of supported architectures can be found
  [here](https://huggingface.co/transformers/pretrained_models.html).
  If left empty, it uses the default model architecture that original Transformers library loads (see table below).

  ```
  +----------------+--------------+-------------------------+
  | Language Model | Parameter    | Default value for       |
  |                | "model_name" | "model_weights"         |
  +----------------+--------------+-------------------------+
  | BERT           | bert         | bert-base-uncased       |
  +----------------+--------------+-------------------------+
  | GPT            | gpt          | openai-gpt              |
  +----------------+--------------+-------------------------+
  | GPT-2          | gpt2         | gpt2                    |
  +----------------+--------------+-------------------------+
  | XLNet          | xlnet        | xlnet-base-cased        |
  +----------------+--------------+-------------------------+
  | DistilBERT     | distilbert   | distilbert-base-uncased |
  +----------------+--------------+-------------------------+
  | RoBERTa        | roberta      | roberta-base            |
  +----------------+--------------+-------------------------+
  ```

  The following configuration loads the language model BERT:

  ```yaml
  pipeline:
    - name: HFTransformersNLP
      # Name of the language model to use
      model_name: "bert"
      # Pre-Trained weights to be loaded
      model_weights: "bert-base-uncased"

      # An optional path to a specific directory to download and cache the pre-trained model weights.
      # The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .
      cache_dir: null
  ```
