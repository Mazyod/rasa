---
id: llm-nlg
sidebar_label: NLG using LLMs
title: LLMs for Natural Language Generation
abstract: |
  Respond to users more naturally by using an LLM to
  rephrase your templated responses, taking the context
  of the conversation into account.
---

import llmNlgExample from "./llm-nlg-example.png";
import RasaLabsLabel from "@theme/RasaLabsLabel";
import RasaLabsBanner from "@theme/RasaLabsBanner";

<RasaLabsLabel />

<RasaLabsBanner />

## Key Features

1. **Dynamic Responses**: By employing the LLM to rephrase static response
   templates, the responses generated by your bot will sound more natural and
   conversational, enhancing user interaction.
2. **Contextual Awareness**: The LLM uses the context and previous conversation
   turns to rephrase the templated response.
3. **Controllable**: By starting with an existing template, we specify what
   the bot will say.
4. **Customizable**: The prompt used for rephrasing can be modified and 
   optimized for your use case.

## Demo

The following example shows a demo of a chatbot using an LLM to rephrase static
response templates. The right and left hand side of the conversation show
the same bot, the only difference is that the left hand side uses an LLM to
rephrase the response templates.

<Image
  img={llmNlgExample}
  caption="Bot response with and without rephrasing enabled"
  alt=""
/>

Rephrasing messages can significantly improve the user experience
and make users feel understood. Consider the different ways a bot might
respond to an out of scope request like “can you order me a pizza?”. 

| response                                                                                                        | comment                                                   |
| --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------|
| I'm sorry, I can't help with that                                                                               | stilted and generic                                       |
| I'm sorry, I can't help you order a pizza                                                                       | acknowledges the user's request                           |
| I can't help you order a pizza, delicious though it is. Do you have any questions related to your account?      | reinforces the assistant's personality                    |

The second and third examples would be difficult to achieve with templates. 

:::note Unchanged interaction flow

Note tht the way the **bot** behaves is not affected by the rephrasing.
Stories, rules, and forms will behave exactly the same way. 
But do be aware that **user** behaviour will often change as a result of the rephrasing.
We recommend regularly reviewing conversations to understand how the user experience
is impacted. 

:::

## How to Use Rephrasing in Your Bot

The following assumes that you have already [configured your NLG server](../nlg.mdx).

To use rephrasing, add the following lines to your `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
  type: rasa_plus.ml.LLMResponseRephraser
```

By default, rephrasing is only enabled for responses that specify
`rephrase: true` in the response template's metadata. To enable rephrasing for a
response, add this property to the response's metadata:

```yaml-rasa title="domain.yml"
responses:
  utter_greet:
    - text: "Hey! How can I help you?"
      metadata:
        rephrase: true
```

If you want to enable rephrasing for all responses, you can set the
`rephrase_all` property to `true` in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
  type: rasa_plus.ml.LLMResponseRephraser
  rephrase_all: true
```


## Customization

You can customize the LLM by modifying the following parameters in the
`endpoints.yml` file.

### Rephrasing all responses

Instead of enabling rephrasing per response, you can enable it for all responses
by setting the `rephrase_all` property to `true` in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    rephrase_all: true
```

Defaults to `false`. Setting this property to `true` will enable rephrasing for
all responses, even if they don't specify `rephrase: true` in the response
metadata. If you want to disable rephrasing for a specific response, you can set
`rephrase: false` in the response metadata.

### Specify an OpenAI Model

You can specify the openai model to use for rephrasing by setting the
`model_name` property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    model_name: text-davinci-003
```

Defaults to `text-davinci-003`. The model name needs to be set to a generative
model using the completions API of
[OpenAI](https://platform.openai.com/docs/guides/gpt/completions-api).

### Temperature

The temperature allows you to control the diversity of the generated responses.
You can specify the temperature to use for rephrasing by setting the
`temperature` property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    temperature: 0.7
```

Defaults to `0.7` (this is the default from OpenAI). The temperature is a value between 0 and 1 that controls the
diversity of the generated responses. Lower temperatures result in more
predictable responses, while higher temperatures result in more variable
responses.

### Prompt

You can change the prompt used to rephrase the response by setting the `prompt`
property in the `endpoints.yml` file:

```yaml-rasa title="endpoints.yml"
nlg:
    type: rasa_plus.ml.LLMResponseRephraser
    prompt: |
        The following is a conversation with
        an AI assistant. The assistant is helpful, creative, clever, and very friendly.
        Rephrase the AI response staying close to the original message and retaining
        its meaning. Use simple english.

        Summary of the conversation:
        {{history}}

        {{current_input}}
        AI Response: {{suggested_response}}

        Rephrased Response:
```

The prompt is a [Jinja2](https://jinja.palletsprojects.com/en/3.0.x/) template
that can be used to customize the prompt. The following variables are available
in the prompt:

- `history`: The conversation history as a summary of the prior conversation,
  e.g.
  ```
  User greeted the assistant.
  ```
- `current_input`: The current user input, e.g.
  ```
  HUMAN: I want to open a bank account
  ```
- `suggested_response`: The suggested response from the LLM. e.g.
  ```
  What type of account would you like to open?
  ```

You can also customize the prompt for a single response by setting the
`rephrase_prompt` property in the response metadata:

```yaml-rasa title="domain.yml"
responses:
  utter_greet:
    - text: "Hey! How can I help you?"
      metadata:
        rephrase: true
        rephrase_prompt: |
            The following is a conversation with
            an AI assistant. The assistant is helpful, creative, clever, and very friendly.
            Rephrase the AI response staying close to the original message and retaining
            its meaning. Use simple english.

            Summary of the conversation:
            {{history}}

            {{current_input}}
            AI Response: {{suggested_response}}

            Rephrased Response:
```

## Observations

Rephrasing responses is a great way to enhance your chatbot's responses. Here
are some observations to keep in mind when using the LLM:

### Success Cases

LLM shows great potential in the following scenarios:

- **Repeated Responses**: When your bot sends the same response twice in a row,
  rephrasing sounds more natural and less robotic.

- **General Conversation**: When users combine a request with a bit of small-talk,
   the LLM will typically echo this behavior.

### Limitations

While the LLM delivers impressive results, there are a few situations where it
may fall short:

- **Structured Responses**: If the template response contains structured
  information (e.g., bullet points), this structure might be lost during
  rephrasing. We are working on resolving this limitation of the current system.

- **Meaning Alteration**: Sometimes, the LLM will not generate a true paraphrase,
   but slightly alter the meaning of the original template. Lowering the temperature
   reduces the likelihood of this happening.
